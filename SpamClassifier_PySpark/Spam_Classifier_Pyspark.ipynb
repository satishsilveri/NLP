{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam Classifier in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using the data that is available publicly on Kaggle. The link is: https://www.kaggle.com/uciml/sms-spam-collection-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I built this code on Databricks Community edition. If you dont have an account, create one by visiting to https://community.cloud.databricks.com/login.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are the steps to set the environment on databricks community edition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Spinup the cluster"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='create_cluster.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Upload the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='add_data.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Table in Notebook\n",
    "\n",
    "Click on Create Table in Notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='create_table.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam Classification using RandomForest Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File location and type\n",
    "file_location = \"/FileStore/tables/SPAM_text_message_20170820___Data-2fd2d.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check the count of the data\n",
    "df.count()\n",
    "\n",
    "# Thats good amount of data we have to train the model. Before training the model, let's do some basic preprocessing w.r.t textual data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Step 1 : Tokenizing the email content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We initialize the tokenizer first and then create a model and then transform the data.\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"Message\", outputCol=\"tokens\")\n",
    "\n",
    "tokenized_df =  tokenizer.transform(df)\n",
    "\n",
    "\n",
    "# Lets see the first row of new df\n",
    "tokenized_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Step 2 : Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We follow the same process again, build stopwords model and transform the tokenized df.\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "stop_word_remover =  StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
    "filtered_df = stop_word_remover.transform(tokenized_df)\n",
    "\n",
    "\n",
    "# Let's check the data. We will set truncate to false.\n",
    "\n",
    "filtered_df.show(1,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Step 3 : Removing Garbage tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see we have some garbage data like tokens having length =1 & 2. So lets filter such tokens using lamba expressions\n",
    "\n",
    "# Register a udf\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def filter_tokens(filtered_tokens):\n",
    "  final_list=[]\n",
    "  for token in filtered_tokens:\n",
    "    if len(token)>=3:\n",
    "      final_list.append(token)\n",
    "  return final_list\n",
    "\n",
    "filter_token_udf = udf(filter_tokens,ArrayType(StringType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column using the created UDF\n",
    "\n",
    "final_filtered_df = filtered_df.withColumn(\"final_filtered_tokens\",filter_token_udf(\"filtered_tokens\"))\n",
    "\n",
    "final_filtered_df.show(1,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the required columns only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we proceed, let select the columns that we need and create a new dataframe\n",
    "new_DF = final_filtered_df.select(\"Category\",\"Message\",\"final_filtered_tokens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Step 4 : Remove special characters from tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the above cell, we can observe that We need to remove special characters from each tokens.\n",
    "\n",
    "\n",
    "#We will use the UDF to build our custom function in python.\n",
    "#Note that all these UDF are built using inbuilt python methods and you do not need any special libraries to be installed.\n",
    "\n",
    "def remove_special_chars(final_filtered_tokens):\n",
    "  final_token_list=[]\n",
    "  for token in final_filtered_tokens:\n",
    "    alphanumeric = [character for character in token if character.isalnum()]\n",
    "    alphanumeric = \"\".join(alphanumeric)\n",
    "    final_token_list.append(alphanumeric)\n",
    "  \n",
    "  return final_token_list\n",
    "    \n",
    "\n",
    "# Register the above function as UDF\n",
    "remove_special_char_udf = udf(remove_special_chars,ArrayType(StringType()))\n",
    "\n",
    "# Apply this UDF to final_filtered_tokens column\n",
    "\n",
    "new_DF = new_DF.withColumn(\"cleaned_tokens\",remove_special_char_udf(\"final_filtered_tokens\"))\n",
    "\n",
    "new_DF.show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that our data is some what clean, we can go ahead an computer vectors for the tokens. We will use TF-IDF and to implement this, we have 2 classes viz. HashingTF and IDF.\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "# Term frequency\n",
    "hashingTF = HashingTF(inputCol=\"cleaned_tokens\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurized_data = hashingTF.transform(new_DF)\n",
    "\n",
    "# Inverse document frequency\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurized_data)\n",
    "rescaled_data = idfModel.transform(featurized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splittig data into train test splits.\n",
    "splits = rescaled_data.randomSplit([0.7,0.3],1)\n",
    "\n",
    "train_data = splits[0]\n",
    "\n",
    "test_data = splits[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train count\n",
    "train_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test count\n",
    "test_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that everything is set, we can train our classification Model. I am planning to use RandomForest classifier but will also experiment with Multilayer Perceptron Classifier. We will use Pipeline to train our model.\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# to convert string categories to integer type.\n",
    "labelIndexer = StringIndexer(inputCol=\"Category\", outputCol=\"categoryIndex\").fit(rescaled_data)\n",
    "\n",
    "# create a random forest classsifier object.\n",
    "rf = RandomForestClassifier(labelCol=\"categoryIndex\", featuresCol=\"features\", numTrees=30)\n",
    "\n",
    "# covert the integers back to category strings.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",labels=labelIndexer.labels)\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[labelIndexer, rf, labelConverter])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the trained model.\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"categoryIndex\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %g\" % (accuracy))\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy = 0.889349\n",
    "Test Error = 0.110651"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "name": "Spam_Classifier",
  "notebookId": 3561895734244154
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
